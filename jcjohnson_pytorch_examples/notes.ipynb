{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implement a two-layer network with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 32897865.442742407\n1 31483252.777447995\n2 30956974.294953004\n3 26765111.66923166\n4 19388711.159411073\n5 11658098.506877884\n6 6393968.990223771\n7 3494521.3523915857\n8 2086107.5829685542\n9 1392892.8611802687\n"
     ]
    }
   ],
   "source": [
    "# 1. generate a 2-layer network with numpy\n",
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(10):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implement a two-layer network with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 26278576.0\n1 24012896.0\n2 27376562.0\n3 32710462.0\n4 35686628.0\n5 31511208.0\n6 21584854.0\n7 11561235.0\n8 5491053.5\n9 2666031.75\n"
     ]
    }
   ],
   "source": [
    "# 2. implement a two-layer network with Pytorch\n",
    "import torch\n",
    "\n",
    "# decide to use cpu or gpu\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda')\n",
    "\n",
    "# N is batch_size, D_in is input dimension;\n",
    "# H is hidden dimension, D_out is output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# create random input and output data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w2 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for i in range(10):\n",
    "    # forward pass: compute y\n",
    "    h = x.mm(w1)\n",
    "    # torch.clamp limits a tensor to its lower and upper boundary\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    # Tensor.item() transforms a Tensor to an int\n",
    "    print(i, loss.item())\n",
    "\n",
    "    # backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    # torch.t is transpose of tensor\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Autograd in Pytorch\n",
    "When defining the forward function, a computational graph is created. Nodes in the graphs are Tensors, and edges are functions that produce output Tensors.\n",
    "If we want to set a Tensor to perform backpropagation, set `a = torch.tensor([1, 2, 3], requires_grad=True). After backpropagation a.grad will be a Tensor that holds the gradient of x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 33327216.0\n1 29294718.0\n2 26060116.0\n3 21083236.0\n4 15052104.0\n5 9628452.0\n6 5851244.5\n7 3593964.75\n8 2341618.5\n9 1645527.125\n10 1239974.0\n11 985208.1875\n12 810859.625\n13 682920.1875\n14 583901.1875\n15 504268.9375\n16 438884.5\n17 384362.8125\n18 338230.40625\n19 298880.625\n20 265072.5625\n21 235857.40625\n22 210491.65625\n23 188371.78125\n24 169008.140625\n25 152022.046875\n26 137076.0\n27 123855.9375\n28 112125.4765625\n29 101694.328125\n30 92398.9140625\n31 84096.4296875\n32 76657.0078125\n33 69980.421875\n34 63974.2578125\n35 58561.03515625\n36 53676.01953125\n37 49272.2734375\n38 45284.71484375\n39 41666.07421875\n40 38376.41796875\n41 35383.7734375\n42 32658.07421875\n43 30170.47265625\n44 27900.673828125\n45 25824.5859375\n46 23922.69140625\n47 22177.962890625\n48 20577.484375\n49 19106.224609375\n50 17753.818359375\n51 16508.3359375\n52 15360.6318359375\n53 14302.2265625\n54 13326.380859375\n55 12425.537109375\n56 11592.2373046875\n57 10821.3125\n58 10107.544921875\n59 9445.5732421875\n60 8831.779296875\n61 8262.0439453125\n62 7734.03662109375\n63 7243.94384765625\n64 6788.10986328125\n65 6363.84423828125\n66 5969.10302734375\n67 5601.47412109375\n68 5258.79833984375\n69 4938.95166015625\n70 4640.43701171875\n71 4361.62060546875\n72 4101.0849609375\n73 3857.516845703125\n74 3629.533203125\n75 3416.093505859375\n76 3216.366455078125\n77 3029.418701171875\n78 2854.165283203125\n79 2689.8818359375\n80 2535.843017578125\n81 2391.313232421875\n82 2255.72265625\n83 2128.328125\n84 2008.6883544921875\n85 1896.282958984375\n86 1790.619384765625\n87 1691.325439453125\n88 1597.9541015625\n89 1510.0577392578125\n90 1427.3167724609375\n91 1349.4320068359375\n92 1276.096435546875\n93 1206.9835205078125\n94 1141.9052734375\n95 1080.5931396484375\n96 1022.7322998046875\n97 968.2060546875\n98 916.7869873046875\n99 868.2155151367188\n"
     ]
    }
   ],
   "source": [
    "# 3. implement a two-layer network with auto-grad\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda')\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# if we want to apply autograd to a Tensor, just set requires_grad = True\n",
    "w1 = torch.randn(D_in, H, requires_grad=True, device=device)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True, device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for i in range(100):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(i, loss.item())\n",
    "    # use loss.backward() can calculate gradient for parameters that are set\n",
    "    # requires_grad=True. After calculation, w1.grad and w2.grad will be Tensors\n",
    "    # that hold gradient for w1 and w2\n",
    "    loss.backward()\n",
    "\n",
    "    # next step is update gradients. this step we don't want to build up a\n",
    "    # computation graph for w1 and w2. so use torch.no_grad() to prevent\n",
    "    # Pytorch from building a computational graph for the updates\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "    # manually set grad to zeros. This step can't skip because w1.grad\n",
    "    # will not clear automatically.\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. define modified autograd functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 37900088.0\n1 35574084.0\n2 35044824.0\n3 30612100.0\n4 21881946.0\n5 12847150.0\n6 6787024.0\n7 3643665.75\n8 2178955.75\n9 1478969.5\n"
     ]
    }
   ],
   "source": [
    "# 4. define modified autograd functions\n",
    "import torch\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a context object and a Tensor containing the\n",
    "        input; we must return a Tensor containing the output, and we can use the\n",
    "        context object to cache objects for use in the backward pass.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(x)\n",
    "        return x.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive the context object and a Tensor containing\n",
    "        the gradient of the loss with respect to the output produced during the\n",
    "        forward pass. We can retrieve cached data from the context object, and must\n",
    "        compute and return the gradient of the loss with respect to the input to the\n",
    "        forward function.\n",
    "        \"\"\"\n",
    "        x, = ctx.saved_tensors\n",
    "        grad_x = grad_output.clone()\n",
    "        grad_x[x < 0] = 0\n",
    "        return grad_x\n",
    "\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and output\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(10):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; we call our\n",
    "    # custom ReLU implementation using the MyReLU.apply function\n",
    "    y_pred = MyReLU.apply(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Update weights using gradient descent\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after running the backward pass\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Pytorch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 684.2965087890625\n1 683.763916015625\n2 683.2320556640625\n3 682.7007446289062\n4 682.1700439453125\n5 681.639892578125\n6 681.1102905273438\n7 680.5814819335938\n8 680.0530395507812\n9 679.5254516601562\n"
     ]
    }
   ],
   "source": [
    "# 5. Implement a two-layer network with torch.nn module\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda')\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# nn.Sequential is a module which contains other modules and\n",
    "# combines them in sequence to generate output. Each linear\n",
    "# module computes output using a linear function.\n",
    "# After building a Sequential model, use .to() method to move\n",
    "# it to cpu or gpu.\n",
    "model = torch.nn.Sequential(torch.nn.Linear(D_in, H),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(H, D_out)\n",
    "                            ).to(device)\n",
    "\n",
    "# reduction is the way that loss_func is calculated among samples.\n",
    "# it's more common to set reduction='elementwise_mean'\n",
    "loss_func = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(10):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_func(y, y_pred)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # remember to zero the gradient before running the back_propagation\n",
    "    model.zero_grad()\n",
    "\n",
    "    # calculate the back propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # update all parameters in model\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.data -= learning_rate * param.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Pytorch:optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.115718960762024\n1 1.0883523225784302\n2 1.0618207454681396\n3 1.03631591796875\n4 1.0116201639175415\n5 0.9877586960792542\n6 0.9646045565605164\n7 0.942121148109436\n8 0.9202743172645569\n9 0.899020791053772\n"
     ]
    }
   ],
   "source": [
    "# 6. optim in Pytorch. Use to replace manually update parameter\n",
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        )\n",
    "loss_fn = torch.nn.MSELoss(reduction='elementwise_mean')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(10):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(i, loss.item())\n",
    "    \n",
    "    # use optimizer.zero_grad() to clear gradident for parameters\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward()\n",
    "    # call the step function to perform a step to update parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. custom nn module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Custom nn Modules\n",
    "import torch\n",
    "\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    # init function defines network structure\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    # forward defines how to calculate output\n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# create random Tensors for input and output\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='elementwise_mean')\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "for i in range(10):\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # compute and print loss func\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(i, loss.item())\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
